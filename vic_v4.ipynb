{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "tweets = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(lower_bound, upper_bound, tweets, ransta=0):\n",
    "    assert lower_bound < upper_bound, \"wrong bounds\"\n",
    "    keep_time = False\n",
    "    X = tweets[tweets.retweet_count >= lower_bound].copy()\n",
    "    X_big = X.copy()\n",
    "    X = X[X.retweet_count < upper_bound]\n",
    "    X_low = tweets[tweets.retweet_count < lower_bound].copy()\n",
    "    y = X.retweet_count.copy()\n",
    "    y_big = X_big.retweet_count.copy()\n",
    "    y_low = X_low.retweet_count.copy()\n",
    "    X = X[['text', 'timestamp', 'user_verified', 'user_statuses_count', 'user_followers_count']]\n",
    "    X_big = X_big[['text', 'timestamp', 'user_verified', 'user_statuses_count', 'user_followers_count']]\n",
    "    X_low = X_low[['text', 'timestamp', 'user_verified', 'user_statuses_count', 'user_followers_count']]\n",
    "    # adding extra features : size of the text\n",
    "    #X.insert(0, 'size_text', X.text.apply(lambda x: len(x)), True)\n",
    "    #X_big.insert(0, 'size_text', X_big.text.apply(lambda x: len(x)), True)\n",
    "    #X_low.insert(0, 'size_text', X_low.text.apply(lambda x: len(x)), True)\n",
    "\n",
    "    # adding extra features : number of hashtags\n",
    "    def count_hashtags_in_text(text):\n",
    "        return text.count('#')\n",
    "    #X.insert(0, 'hashtag_count', X.text.apply(count_hashtags_in_text), True)\n",
    "    #X_big.insert(0, 'hashtag_count', X_big.text.apply(count_hashtags_in_text), True)\n",
    "    #X_low.insert(0, 'hashtag_count', X_low.text.apply(count_hashtags_in_text), True)\n",
    "\n",
    "    # Converting timestamp in hour\n",
    "    def timestamp_13_digits_to_hour(t):\n",
    "        dt = datetime.fromtimestamp(t / 1000)\n",
    "        return dt.hour\n",
    "    #X.timestamp = X.timestamp.apply(timestamp_13_digits_to_hour)\n",
    "    #X_big.timestamp = X_big.timestamp.apply(timestamp_13_digits_to_hour)\n",
    "    #X_low.timestamp = X_low.timestamp.apply(timestamp_13_digits_to_hour)\n",
    "    #keep_time = True\n",
    "\n",
    "    # Converting the True / False values of user_verified into 1 / 0\n",
    "    X['user_verified'] = (X['user_verified']).astype(int)\n",
    "    X_big['user_verified'] = (X_big['user_verified']).astype(int)\n",
    "    X_low['user_verified'] = (X_low['user_verified']).astype(int)\n",
    "\n",
    "    # Remove text feature\n",
    "    X.drop('text', axis=1, inplace=True)\n",
    "    X_big.drop('text', axis=1, inplace=True)\n",
    "    X_low.drop('text', axis=1, inplace=True)\n",
    "    # Eventually remove timestamp\n",
    "    if not keep_time:\n",
    "        X.drop('timestamp', axis=1, inplace=True)\n",
    "        X_big.drop('timestamp', axis=1, inplace=True)\n",
    "        X_low.drop('timestamp', axis=1, inplace=True)\n",
    "    # Mapping extreme datas to a smaller interval\n",
    "    if 'hashtag_count' in X.columns:\n",
    "        X.loc[X.hashtag_count > 5, 'hashtag_count'] = 5\n",
    "        X_big.loc[X_big.hashtag_count > 5, 'hashtag_count'] = 5\n",
    "    if 'size_text' in X.columns:\n",
    "        X.loc[X.size_text > 400, 'size_text'] = 400\n",
    "        X_big.loc[X_big.size_text > 400, 'size_text'] = 400\n",
    "    # Taking the log of some datas\n",
    "    X.user_statuses_count = np.log(1 + X.user_statuses_count)\n",
    "    X.user_followers_count = np.log(1 + X.user_followers_count)\n",
    "    X_big.user_statuses_count = np.log(1 + X_big.user_statuses_count)\n",
    "    X_big.user_followers_count = np.log(1 + X_big.user_followers_count)\n",
    "    \n",
    "    return X, X_big, X_low, y, y_big, y_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_normalize(X, X_big):\n",
    "    X = X - X.mean()\n",
    "    scaler = MinMaxScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    X_big = X_big - X_big.mean()\n",
    "    scaler = MinMaxScaler()\n",
    "    X_big = pd.DataFrame(scaler.fit_transform(X_big), columns=X_big.columns)\n",
    "    \n",
    "    return X, X_big\n",
    "\n",
    "def prepare_data(lower_bound, upper_bound, tweets, ransta=0):\n",
    "    X, X_big, X_low, y, y_big, y_low = select_features(lower_bound, upper_bound, tweets, ransta)\n",
    "    X, X_big = center_normalize(X, X_big)\n",
    "    \n",
    "    return X, X_big, X_low, y, y_big, y_low\n",
    "\n",
    "def cv_worst(estimators, X, y, verbose=True):\n",
    "    mae_worst = 0\n",
    "    esti_worst = None\n",
    "    for esti in estimators:\n",
    "        y_pred = esti.predict(X).astype(int)\n",
    "        cur_mae = mean_absolute_error(y_true=y, y_pred=y_pred)\n",
    "        if cur_mae > mae_worst:\n",
    "            mae_worst = cur_mae\n",
    "            esti_worst = esti\n",
    "    if verbose:\n",
    "        print(\"Prediction error :\", mae_worst)\n",
    "    \n",
    "    return esti_worst\n",
    "\n",
    "def error_true(predictor, X_big, y_big, X_low, y_low):\n",
    "    y_low_pred = y_low * 0\n",
    "    y_big_pred = predictor.predict(X_big).astype(int)\n",
    "    y_pred = np.append(y_low_pred, y_big_pred)\n",
    "    y_true = np.append(y_low, y_big)\n",
    "    mae = mean_absolute_error(y_true=y_true, y_pred=y_pred)\n",
    "    print(\"Prediction error on true datas :\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal : beat the constant 0 prediction !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant 0 prediction error : 147.6873983330755\n"
     ]
    }
   ],
   "source": [
    "print(\"Constant 0 prediction error :\", mean_absolute_error(tweets.retweet_count, tweets.retweet_count*0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to run the linear regression for different bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 290 , 1000 ] :Prediction error on true datas : 133.055533609602\n",
      "[ 290 , 1200 ] :Prediction error on true datas : 132.40877801426004\n",
      "[ 290 , 1400 ] :Prediction error on true datas : 131.95592968816885\n",
      "[ 290 , 1600 ] :Prediction error on true datas : 131.62538657838886\n",
      "[ 290 , 1800 ] :Prediction error on true datas : 131.3451486008078\n",
      "[ 290 , 2000 ] :Prediction error on true datas : 131.16585883862012\n",
      "[ 290 , 2200 ] :Prediction error on true datas : 131.0211977884487\n",
      "[ 290 , 2400 ] :Prediction error on true datas : 130.9324773910784\n",
      "[ 290 , 2600 ] :Prediction error on true datas : 130.8701847615643\n",
      "[ 290 , 2800 ] :Prediction error on true datas : 130.81501163302428\n",
      "[ 290 , 3000 ] :Prediction error on true datas : 130.78782234892464\n",
      "[ 290 , 3200 ] :Prediction error on true datas : 130.7825127632826\n",
      "[ 290 , 3400 ] :Prediction error on true datas : 130.80162576958952\n",
      "[ 290 , 3600 ] :Prediction error on true datas : 130.82281454601164\n",
      "[ 290 , 3800 ] :Prediction error on true datas : 130.8710769521927\n",
      "[ 290 , 4000 ] :Prediction error on true datas : 130.92311539749795\n",
      "\n",
      "[ 300 , 1000 ] :Prediction error on true datas : 132.9712981974445\n",
      "[ 300 , 1200 ] :Prediction error on true datas : 132.3048933802159\n",
      "[ 300 , 1400 ] :Prediction error on true datas : 131.83677868115\n",
      "[ 300 , 1600 ] :Prediction error on true datas : 131.49336489545297\n",
      "[ 300 , 1800 ] :Prediction error on true datas : 131.2005461288089\n",
      "[ 300 , 2000 ] :Prediction error on true datas : 131.01041189467344\n",
      "[ 300 , 2200 ] :Prediction error on true datas : 130.85542306207634\n",
      "[ 300 , 2400 ] :Prediction error on true datas : 130.7581172074133\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-15a537f2808f>\u001b[0m in \u001b[0;36mprepare_data\u001b[1;34m(lower_bound, upper_bound, tweets, ransta)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mransta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_big\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_low\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_big\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_low\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mransta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_big\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcenter_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_big\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-84c8f272c66d>\u001b[0m in \u001b[0;36mselect_features\u001b[1;34m(lower_bound, upper_bound, tweets, ransta)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlower_bound\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wrong bounds\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mkeep_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretweet_count\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mX_big\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretweet_count\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m             \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres_values\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for lower_bound in range(290, 500, 10):\n",
    "    for upper_bound in range(1000, 4002, 200):\n",
    "        X, X_big, X_low, y, y_big, y_low = prepare_data(lower_bound, upper_bound, tweets)\n",
    "        # Applying linear regression\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        lr = LinearRegression(fit_intercept=False)\n",
    "        lr_estimators = cross_validate(lr, X, y, return_estimator=True)['estimator']\n",
    "        lr = cv_worst(lr_estimators, X, y, verbose=False)\n",
    "        print('[', lower_bound, ',', upper_bound, \"] :\", end='')\n",
    "        error_true(lr, X_big, y_big, X_low, y_low)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ransta = 0\n",
    "lower_bound = 10\n",
    "upper_bound = 1000\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=ransta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying SGD regression with huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 99.73031627040454\n",
      "Prediction error on true datas : 144.09929000250835\n",
      "Wall time: 5.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd = SGDRegressor(max_iter=1000, tol=1e-4, random_state=ransta, loss='huber')\n",
    "sgd_estimators = cross_validate(sgd, X, y, return_estimator=True)['estimator']\n",
    "sgd = cv_worst(sgd_estimators, X, y)\n",
    "error_true(sgd, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 117.89967397090135\n",
      "Prediction error on true datas : 144.8003280377664\n",
      "Wall time: 178 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "lr_estimators = cross_validate(lr, X, y, return_estimator=True)['estimator']\n",
    "lr = cv_worst(lr_estimators, X, y)\n",
    "error_true(lr, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying ridge regression : it does not improve the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 117.90016190560681\n",
      "Prediction error on true datas : 144.80072156292573\n",
      "Wall time: 173 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=0.5, fit_intercept=False, random_state=ransta)\n",
    "ridge_estimators = cross_validate(ridge, X, y, return_estimator=True)['estimator']\n",
    "ridge = cv_worst(ridge_estimators, X, y)\n",
    "error_true(ridge, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying bayesian ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 117.89957416607523\n",
      "Prediction error on true datas : 144.80068851882837\n",
      "Wall time: 203 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "bay = BayesianRidge(fit_intercept=False)\n",
    "bay_estimators = cross_validate(bay, X, y, return_estimator=True)['estimator']\n",
    "bay = cv_worst(bay_estimators, X, y)\n",
    "error_true(bay, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying generalized linear regression Power. Distribution\n",
    "0 : Normal.\n",
    "1 : Poisson.\n",
    "2 : Gamma.\n",
    "3 : Inverse Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 104.27812278211498\n",
      "Prediction error on true datas : 144.67818203392426\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "gen = TweedieRegressor(power=2, alpha=1, link='log', fit_intercept=False)\n",
    "gen_estimators = cross_validate(gen, X, y, return_estimator=True)['estimator']\n",
    "gen = cv_worst(gen_estimators, X, y)\n",
    "error_true(gen, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying passive aggressive regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 98.82284643364088\n",
      "Prediction error on true datas : 143.9007730816775\n",
      "Wall time: 797 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "par = PassiveAggressiveRegressor(max_iter=100, random_state=ransta, tol=1e-3, fit_intercept=False)\n",
    "par_estimators = cross_validate(par, X, y, return_estimator=True)['estimator']\n",
    "par = cv_worst(par_estimators, X, y)\n",
    "error_true(par, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying gamma regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 117.18571460255501\n",
      "Prediction error on true datas : 144.71702537035674\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import GammaRegressor\n",
    "gam = GammaRegressor(alpha=0.001)\n",
    "gam_estimators = cross_validate(gam, X, y, return_estimator=True)['estimator']\n",
    "gam = cv_worst(gam_estimators, X, y)\n",
    "error_true(gam, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying poisson regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 116.97436124911285\n",
      "Prediction error on true datas : 144.53743520728412\n",
      "Wall time: 910 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "poi = PoissonRegressor(alpha=0.001)\n",
    "poi_estimators = cross_validate(poi, X, y, return_estimator=True)['estimator']\n",
    "poi = cv_worst(poi_estimators, X, y)\n",
    "error_true(poi, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying k-nn regression : why does it fool the cross validation ??? Very sensitive to feature change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 27.86419889992903\n",
      "Prediction error on true datas : 144.75071532960737\n",
      "\n",
      "Prediction error on a train / test split : 126.74427974716298\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(n_neighbors=1)\n",
    "knn_estimators = cross_validate(knn, X, y, return_estimator=True)['estimator']\n",
    "knn = cv_worst(knn_estimators, X, y)\n",
    "error_true(knn, X_big, y_big, X_low, y_low)\n",
    "knn.fit(X_train, y_train)\n",
    "p = knn.predict(X_test).astype(int)\n",
    "print(\"\\nPrediction error on a train / test split :\", mean_absolute_error(y_true=y_test, y_pred=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying logistic regression - I am not sure about what it does and how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction error on a train / test split : 111.02007171108565\n",
      "Prediction error on true datas : 146.0392008134856\n",
      "Wall time: 194 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "sample = 100\n",
    "logi = LogisticRegression() # penalty='l1', solver='saga', random_state=0, fit_intercept=False)\n",
    "logi.fit(X_train[:sample], y_train[:sample])\n",
    "p = logi.predict(X_test).astype(int)\n",
    "print(\"\\nPrediction error on a train / test split :\", mean_absolute_error(y_true=y_test, y_pred=p))\n",
    "error_true(logi, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying gradient boosting regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 94.91043071327182\n",
      "Prediction error on true datas : 142.69297527550515\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(loss='lad', criterion='friedman_mse', random_state=ransta)\n",
    "gbr_estimators = cross_validate(gbr, X, y, return_estimator=True)['estimator']\n",
    "gbr = cv_worst(gbr_estimators, X, y)\n",
    "error_true(gbr, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying neural network (MLP regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error on a train / test split : 98.47406941928807\n",
      "Prediction error on true datas : 143.59846915709014\n",
      "Wall time: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "mlp = Sequential()\n",
    "input_shape = X.iloc[0].shape\n",
    "mlp.add(Dense(16, input_shape=input_shape, activation='relu'))\n",
    "mlp.add(Dense(8, activation='relu'))\n",
    "mlp.add(Dense(1, activation='linear'))\n",
    "mlp.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "mlp.fit(X_test, y_test, epochs=10, batch_size=10, verbose=0, validation_split=0.2)\n",
    "p = mlp.predict(X_test).astype(int)\n",
    "print(\"Prediction error on a train / test split :\", mean_absolute_error(y_true=y_test, y_pred=p))\n",
    "error_true(mlp, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying SVM regression (too long, but seems to have nice results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 97.06281050390348\n",
      "Prediction error on true datas : 143.40786930158296\n",
      "Wall time: 39.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import svm\n",
    "sample = 5000\n",
    "svmr = svm.SVR(kernel='poly')\n",
    "svmr_estimators = cross_validate(svmr, X[:sample], y[:sample], return_estimator=True)['estimator']\n",
    "svmr = cv_worst(svmr_estimators, X, y)\n",
    "error_true(svmr, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying random forest regression with mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 117.40592840667139\n",
      "Prediction error on true datas : 144.85139018019547\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=2, random_state=ransta, criterion='mse')\n",
    "rf_estimators = cross_validate(rf, X, y, return_estimator=True)['estimator']\n",
    "rf = cv_worst(rf_estimators, X, y)\n",
    "rf_pred = (rf.predict(X_test)).astype(int)\n",
    "error_true(rf, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying random forest regression with mae - poor implementation in sklearn with complexity in O(n^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=1, max_depth=1, random_state=ransta, criterion='mae')\n",
    "rf_estimators = cross_validate(rf, X, y, return_estimator=True)['estimator']\n",
    "rf = cv_worst(rf_estimators, X, y)\n",
    "rf_pred = (rf.predict(X_test)).astype(int)\n",
    "val, occ = np.unique(rf_pred, return_counts=True)\n",
    "print('\\n', val, \"with respecting occurences\", occ)\n",
    "error_true(rf, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.insert(3, 'stat_sqr', X.user_statuses_count ** 2, True)\n",
    "#X.insert(4, 'followers_sqr', X.user_followers_count ** 2, True)\n",
    "#X.insert(5, 'verified_exp', np.exp(X.user_verified), True)\n",
    "\n",
    "#X_big.insert(3, 'stat_sqr', X_big.user_statuses_count ** 2, True)\n",
    "#X_big.insert(4, 'followers_sqr', X_big.user_followers_count ** 2, True)\n",
    "#X_big.insert(5, 'verified_exp', np.exp(X_big.user_verified), True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
