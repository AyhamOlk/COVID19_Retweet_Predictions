{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "tweets = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(lower_bound, upper_bound, tweets, ransta=0):\n",
    "    assert lower_bound < upper_bound, \"wrong bounds\"\n",
    "    keep_time = False\n",
    "    X = tweets[tweets.retweet_count >= lower_bound].copy()\n",
    "    X_big = X.copy()\n",
    "    X = X[X.retweet_count < upper_bound]\n",
    "    X_low = tweets[tweets.retweet_count < lower_bound].copy()\n",
    "    y = X.retweet_count.copy()\n",
    "    y_big = X_big.retweet_count.copy()\n",
    "    y_low = X_low.retweet_count.copy()\n",
    "    X = X[['text', 'timestamp', 'user_verified', 'user_statuses_count', 'user_followers_count']]\n",
    "    X_big = X_big[['text', 'timestamp', 'user_verified', 'user_statuses_count', 'user_followers_count']]\n",
    "    X_low = X_low[['text', 'timestamp', 'user_verified', 'user_statuses_count', 'user_followers_count']]\n",
    "    # adding extra features : size of the text\n",
    "    #X.insert(0, 'size_text', X.text.apply(lambda x: len(x)), True)\n",
    "    #X_big.insert(0, 'size_text', X_big.text.apply(lambda x: len(x)), True)\n",
    "    #X_low.insert(0, 'size_text', X_low.text.apply(lambda x: len(x)), True)\n",
    "\n",
    "    # adding extra features : number of hashtags\n",
    "    def count_hashtags_in_text(text):\n",
    "        return text.count('#')\n",
    "    #X.insert(0, 'hashtag_count', X.text.apply(count_hashtags_in_text), True)\n",
    "    #X_big.insert(0, 'hashtag_count', X_big.text.apply(count_hashtags_in_text), True)\n",
    "    #X_low.insert(0, 'hashtag_count', X_low.text.apply(count_hashtags_in_text), True)\n",
    "\n",
    "    # Converting timestamp in hour\n",
    "    def timestamp_13_digits_to_hour(t):\n",
    "        dt = datetime.fromtimestamp(t / 1000)\n",
    "        return dt.hour\n",
    "    #X.timestamp = X.timestamp.apply(timestamp_13_digits_to_hour)\n",
    "    #X_big.timestamp = X_big.timestamp.apply(timestamp_13_digits_to_hour)\n",
    "    #X_low.timestamp = X_low.timestamp.apply(timestamp_13_digits_to_hour)\n",
    "    #keep_time = True\n",
    "\n",
    "    # Converting the True / False values of user_verified into 1 / 0\n",
    "    X['user_verified'] = (X['user_verified']).astype(int)\n",
    "    X_big['user_verified'] = (X_big['user_verified']).astype(int)\n",
    "    X_low['user_verified'] = (X_low['user_verified']).astype(int)\n",
    "\n",
    "    # Remove text feature\n",
    "    X.drop('text', axis=1, inplace=True)\n",
    "    X_big.drop('text', axis=1, inplace=True)\n",
    "    X_low.drop('text', axis=1, inplace=True)\n",
    "    # Eventually remove timestamp\n",
    "    if not keep_time:\n",
    "        X.drop('timestamp', axis=1, inplace=True)\n",
    "        X_big.drop('timestamp', axis=1, inplace=True)\n",
    "        X_low.drop('timestamp', axis=1, inplace=True)\n",
    "    # Mapping extreme datas to a smaller interval\n",
    "    if 'hashtag_count' in X.columns:\n",
    "        X.loc[X.hashtag_count > 5, 'hashtag_count'] = 5\n",
    "        X_big.loc[X_big.hashtag_count > 5, 'hashtag_count'] = 5\n",
    "    if 'size_text' in X.columns:\n",
    "        X.loc[X.size_text > 400, 'size_text'] = 400\n",
    "        X_big.loc[X_big.size_text > 400, 'size_text'] = 400\n",
    "    # Taking the log of some datas\n",
    "    X.user_statuses_count = np.log(1 + X.user_statuses_count)\n",
    "    X.user_followers_count = np.log(1 + X.user_followers_count)\n",
    "    X_big.user_statuses_count = np.log(1 + X_big.user_statuses_count)\n",
    "    X_big.user_followers_count = np.log(1 + X_big.user_followers_count)\n",
    "    \n",
    "    return X, X_big, X_low, y, y_big, y_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_normalize(X, X_big):\n",
    "    X = X - X.mean()\n",
    "    scaler = MinMaxScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    X_big = X_big - X_big.mean()\n",
    "    scaler = MinMaxScaler()\n",
    "    X_big = pd.DataFrame(scaler.fit_transform(X_big), columns=X_big.columns)\n",
    "    \n",
    "    return X, X_big\n",
    "\n",
    "def prepare_data(lower_bound, upper_bound, tweets, ransta=0):\n",
    "    X, X_big, X_low, y, y_big, y_low = select_features(lower_bound, upper_bound, tweets, ransta)\n",
    "    X, X_big = center_normalize(X, X_big)\n",
    "    \n",
    "    return X, X_big, X_low, y, y_big, y_low\n",
    "\n",
    "def cv_worst(estimators, X, y, verbose=True):\n",
    "    mae_worst = 0\n",
    "    esti_worst = None\n",
    "    for esti in estimators:\n",
    "        y_pred = esti.predict(X).astype(int)\n",
    "        cur_mae = mean_absolute_error(y_true=y, y_pred=y_pred)\n",
    "        if cur_mae > mae_worst:\n",
    "            mae_worst = cur_mae\n",
    "            esti_worst = esti\n",
    "    if verbose:\n",
    "        print(\"Prediction error :\", mae_worst)\n",
    "    \n",
    "    return esti_worst\n",
    "\n",
    "def error_true(predictor, X_big, y_big, X_low, y_low):\n",
    "    y_low_pred = y_low * 0\n",
    "    y_big_pred = predictor.predict(X_big).astype(int)\n",
    "    y_pred = np.append(y_low_pred, y_big_pred)\n",
    "    y_true = np.append(y_low, y_big)\n",
    "    mae = mean_absolute_error(y_true=y_true, y_pred=y_pred)\n",
    "    print(\"Prediction error on true datas :\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal : beat the constant 0 prediction !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant 0 prediction error : 147.6873983330755\n"
     ]
    }
   ],
   "source": [
    "print(\"Constant 0 prediction error :\", mean_absolute_error(tweets.retweet_count, tweets.retweet_count*0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def cluster(dataset, threshold):\n",
    "    \"\"\"\n",
    "    Returns a copy of the parameter dataset, with an additional feature 'classes' (0, 1)\n",
    "    \"\"\"\n",
    "    modified = pd.read_csv(\"data/train.csv\")\n",
    "    modified.user_verified = (modified.user_verified).astype(int)\n",
    "    modified.user_mentions = modified.user_mentions.fillna(0)\n",
    "    modified.user_mentions = modified.user_mentions.apply(lambda x: 1 if type(x) is str and x!=0 else 0) \n",
    "    modified.urls = modified.urls.fillna(0)\n",
    "\n",
    "    modified.urls = modified.urls.apply(lambda x: 1 if type(x) is str and x!=0 else 0) \n",
    "    modified.hashtags = modified.hashtags.fillna(0)\n",
    "\n",
    "    modified.hashtags = modified.hashtags.apply(lambda x: 1 if type(x) is str and x!=0 else 0) \n",
    "    modified.text = modified.text.fillna(0)\n",
    "\n",
    "    modified.text = modified.text.apply(lambda x: len(x) if type(x) is str and x!=0 else 0) \n",
    "\n",
    "    x_less_300 = modified[modified.retweet_count<=threshold]\n",
    "    x_greater_300 = modified[modified.retweet_count>threshold]\n",
    "\n",
    "    x_sampled = pd.concat([x_less_300.sample(n=21112, random_state=1), x_greater_300], axis=0)\n",
    "\n",
    "    y_sampled = x_sampled.retweet_count\n",
    "    y_sampled = y_sampled.apply(lambda x: 1 if x>threshold else 0)\n",
    "\n",
    "    x_sampled = x_sampled.drop('retweet_count', 1)\n",
    "\n",
    "    #based on my test 2 was good\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf.fit(x_sampled, y_sampled)\n",
    "\n",
    "    # test\n",
    "    #validation_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "    validation_data = dataset.copy()\n",
    "\n",
    "    if 'retweet_count' in dataset.columns:\n",
    "        validation_data = validation_data.drop('retweet_count', axis=1)\n",
    "\n",
    "    validation_data.user_verified = (validation_data.user_verified).astype(int)\n",
    "    validation_data.user_mentions = validation_data.user_mentions.fillna(0)\n",
    "    validation_data.user_mentions = validation_data.user_mentions.apply(lambda x: 1 if type(x) is str and x!=0 else 0) \n",
    "    validation_data.urls = validation_data.urls.fillna(0)\n",
    "\n",
    "    validation_data.urls = validation_data.urls.apply(lambda x: 1 if type(x) is str and x!=0 else 0) \n",
    "    validation_data.hashtags = validation_data.hashtags.fillna(0)\n",
    "\n",
    "    validation_data.hashtags = validation_data.hashtags.apply(lambda x: 1 if type(x) is str and x!=0 else 0) \n",
    "    validation_data.text = validation_data.text.fillna(0)\n",
    "\n",
    "    validation_data.text = validation_data.text.apply(lambda x: len(x) if type(x) is str and x!=0 else 0)\n",
    "    \n",
    "    y_classes = clf.predict(validation_data) \n",
    "    #eval_dataset['classes'] = y_classes.tolist()\n",
    "    tmp = dataset.copy()\n",
    "    tmp['classes'] = y_classes.tolist()\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(X_val_big_ori):\n",
    "    X_val_big = X_val_big_ori[['text', 'timestamp', 'user_verified', 'user_statuses_count', 'user_followers_count']].copy()\n",
    "    # adding extra features : size of the text\n",
    "    #X_val_big.insert(0, 'size_text', X_val_big.text.apply(lambda x: len(x)), True)\n",
    "    # adding extra features : number of hashtags\n",
    "    def count_hashtags_in_text(text):\n",
    "        return text.count('#')\n",
    "    #X_val_big.insert(0, 'hashtag_count', X_val_big.text.apply(count_hashtags_in_text), True)\n",
    "    # Converting timestamp in hour\n",
    "    def timestamp_13_digits_to_hour(t):\n",
    "        dt = datetime.fromtimestamp(t / 1000)\n",
    "        return dt.hour\n",
    "    #X_val_big.timestamp = X_va_big.timestamp.apply(timestamp_13_digits_to_hour)\n",
    "    # Converting the True / False values of user_verified into 1 / 0\n",
    "    X_val_big['user_verified'] = (X_val_big['user_verified']).astype(int)\n",
    "    # Remove text feature\n",
    "    X_val_big.drop('text', axis=1, inplace=True)\n",
    "    # Eventually remove timestamp\n",
    "    X_val_big.drop('timestamp', axis=1, inplace=True)\n",
    "    # Mapping extreme datas to a smaller interval\n",
    "    if 'hashtag_count' in X.columns:\n",
    "        X_val_big.loc[X_val_big.hashtag_count > 5, 'hashtag_count'] = 5\n",
    "    if 'size_text' in X.columns:\n",
    "        X_val_big.loc[X_val_big.size_text > 400, 'size_text'] = 400\n",
    "    # Taking the log of some datas\n",
    "    X_val_big.user_statuses_count = np.log(1 + X_val_big.user_statuses_count)\n",
    "    X_val_big.user_followers_count = np.log(1 + X_val_big.user_followers_count)\n",
    "    \n",
    "    return X_val_big\n",
    "\n",
    "def center_normalize_X(X_ori):\n",
    "    X = X_ori.copy()\n",
    "    X = X - X.mean()\n",
    "    scaler = MinMaxScaler()\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50 , 2000 ] :Prediction error on true datas : 142.28858311416585\n"
     ]
    }
   ],
   "source": [
    "X, X_big, X_low, y, y_big, y_low = prepare_data(20, 300, tweets)\n",
    "# Applying linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "lr_estimators = cross_validate(lr, X, y, return_estimator=True)['estimator']\n",
    "lr = cv_worst(lr_estimators, X, y, verbose=False)\n",
    "print('[', lower_bound, ',', upper_bound, \"] :\", end='')\n",
    "error_true(lr, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"data/train.csv\")\n",
    "tweets2 = cluster(ds, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26308, 11)"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[tweets.retweet_count > 200].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 : (636793, 12)\n",
      "class 1 : (28984, 12)\n",
      "Prediction error on true datas : 146.76942036006326\n"
     ]
    }
   ],
   "source": [
    "X_val_big = tweets2[tweets2.classes == 1].copy()\n",
    "X_val_low = tweets2[tweets2.classes == 0].copy()\n",
    "print(\"class 0 :\", X_val_low.shape)\n",
    "print(\"class 1 :\", X_val_big.shape)\n",
    "y_true_big = tweets2[tweets2.classes == 1].retweet_count.copy()\n",
    "y_true_low = tweets2[tweets2.classes == 0].retweet_count.copy()\n",
    "X_id_big = X_val_big.id.copy()\n",
    "X_val_big = transform(X_val_big)\n",
    "X_val_big = center_normalize_X(X_val_big)\n",
    "\n",
    "model = lr\n",
    "y_pred_low = X_val_low.id * 0 # constant zero prediction\n",
    "y_pred_big = model.predict(X_val_big)\n",
    "y_true = np.append(y_true_low, y_true_big)\n",
    "y_pred = np.append(y_pred_low, y_pred_big)\n",
    "mae = mean_absolute_error(y_true=y_true, y_pred=y_pred)\n",
    "print(\"Prediction error on true datas :\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 : (274972, 11)\n",
      "class 1 : (10362, 11)\n"
     ]
    }
   ],
   "source": [
    "X_val_big = tweets2[tweets2.classes == 1].copy()\n",
    "X_val_low = tweets2[tweets2.classes == 0].copy()\n",
    "print(\"class 0 :\", X_val_low.shape)\n",
    "print(\"class 1 :\", X_val_big.shape)\n",
    "X_id_big = X_val_big.id.copy()\n",
    "X_val_big = transform(X_val_big)\n",
    "X_val_big = center_normalize_X(X_val_big)\n",
    "\n",
    "model = gbr\n",
    "y_pred_low = X_val_low.id * 0 # constant zero prediction\n",
    "y_pred_big = model.predict(X_val_big)\n",
    "\n",
    "with open(\"predictions.txt\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"TweetID\", \"NoRetweets\"])\n",
    "    for index, prediction in enumerate(y_pred_low):\n",
    "        writer.writerow([str(X_val_low['id'].iloc[index]) , str(int(prediction))])\n",
    "    for index, prediction in enumerate(y_pred_big):\n",
    "        writer.writerow([str(X_id_big.iloc[index]) , str(int(prediction))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr\n",
    "# X_val_big = eval_data with more than split retweets\n",
    "# X_val_low = eval_data with less than split retweets\n",
    "y_pred_low = X_val_low.id * 0 # constant zero prediction\n",
    "\n",
    "y_pred_big = model.predict(X_val_big)\n",
    "\n",
    "with open(\"predictions.txt\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"TweetID\", \"NoRetweets\"])\n",
    "    for index, prediction in enumerate(y_pred_low):\n",
    "        writer.writerow([str(X_val_low['id'].iloc[index]) , str(int(prediction))])\n",
    "    for index, prediction in enumerate(y_pred_big):\n",
    "        writer.writerow([str(X_id_big.iloc[index]) , str(int(prediction))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4500 , 25000 ] :Prediction error on true datas : 113.04612505388441\n",
      "[ 4500 , 27000 ] :Prediction error on true datas : 113.02099501785132\n",
      "[ 4500 , 29000 ] :Prediction error on true datas : 112.95723342800967\n",
      "[ 4500 , 31000 ] :Prediction error on true datas : 112.9805805847904\n",
      "[ 4500 , 33000 ] :Prediction error on true datas : 113.0552737628365\n",
      "[ 4500 , 35000 ] :Prediction error on true datas : 113.1783645274619\n",
      "\n",
      "[ 4600 , 25000 ] :Prediction error on true datas : 112.97626983209092\n",
      "[ 4600 , 27000 ] :Prediction error on true datas : 112.93824208406117\n",
      "[ 4600 , 29000 ] :Prediction error on true datas : 112.8706969450732\n",
      "[ 4600 , 31000 ] :Prediction error on true datas : 112.8860639523444\n",
      "[ 4600 , 33000 ] :Prediction error on true datas : 112.9545208080183\n",
      "[ 4600 , 35000 ] :Prediction error on true datas : 113.06285287716457\n",
      "\n",
      "[ 4700 , 25000 ] :Prediction error on true datas : 112.90294648208034\n",
      "[ 4700 , 27000 ] :Prediction error on true datas : 112.85180923943003\n",
      "[ 4700 , 29000 ] :Prediction error on true datas : 112.76540943889621\n",
      "[ 4700 , 31000 ] :Prediction error on true datas : 112.76686187717509\n",
      "[ 4700 , 33000 ] :Prediction error on true datas : 112.8169552267501\n",
      "[ 4700 , 35000 ] :Prediction error on true datas : 112.91643297981156\n",
      "\n",
      "[ 4800 , 25000 ] :Prediction error on true datas : 112.81328733194448\n",
      "[ 4800 , 27000 ] :Prediction error on true datas : 112.75315758880225\n",
      "[ 4800 , 29000 ] :Prediction error on true datas : 112.65859439421908\n",
      "[ 4800 , 31000 ] :Prediction error on true datas : 112.64139043553622\n",
      "[ 4800 , 33000 ] :Prediction error on true datas : 112.68779636424809\n",
      "[ 4800 , 35000 ] :Prediction error on true datas : 112.7731702957597\n",
      "\n",
      "[ 4900 , 25000 ] :Prediction error on true datas : 112.75036386057194\n",
      "[ 4900 , 27000 ] :Prediction error on true datas : 112.68311161244681\n",
      "[ 4900 , 29000 ] :Prediction error on true datas : 112.57714820427861\n",
      "[ 4900 , 31000 ] :Prediction error on true datas : 112.55092170501534\n",
      "[ 4900 , 33000 ] :Prediction error on true datas : 112.57967006970803\n",
      "[ 4900 , 35000 ] :Prediction error on true datas : 112.66015647882098\n",
      "\n",
      "[ 5000 , 25000 ] :Prediction error on true datas : 112.68174629042457\n",
      "[ 5000 , 27000 ] :Prediction error on true datas : 112.60191625724529\n",
      "[ 5000 , 29000 ] :Prediction error on true datas : 112.47747819465076\n",
      "[ 5000 , 31000 ] :Prediction error on true datas : 112.43802354241735\n",
      "[ 5000 , 33000 ] :Prediction error on true datas : 112.44847298720143\n",
      "[ 5000 , 35000 ] :Prediction error on true datas : 112.52071189001722\n",
      "\n",
      "[ 5100 , 25000 ] :Prediction error on true datas : 112.59584665736425\n",
      "[ 5100 , 27000 ] :Prediction error on true datas : 112.50739511878602\n",
      "[ 5100 , 29000 ] :Prediction error on true datas : 112.37304082297827\n",
      "[ 5100 , 31000 ] :Prediction error on true datas : 112.32119313223497\n",
      "[ 5100 , 33000 ] :Prediction error on true datas : 112.3203204676641\n",
      "[ 5100 , 35000 ] :Prediction error on true datas : 112.38650028462983\n",
      "\n",
      "[ 5200 , 25000 ] :Prediction error on true datas : 112.58231359749585\n",
      "[ 5200 , 27000 ] :Prediction error on true datas : 112.48944766791283\n",
      "[ 5200 , 29000 ] :Prediction error on true datas : 112.3452101829892\n",
      "[ 5200 , 31000 ] :Prediction error on true datas : 112.2866740665418\n",
      "[ 5200 , 33000 ] :Prediction error on true datas : 112.28115870629355\n",
      "[ 5200 , 35000 ] :Prediction error on true datas : 112.33499054488215\n",
      "\n",
      "[ 5300 , 25000 ] :Prediction error on true datas : 112.51425477299456\n",
      "[ 5300 , 27000 ] :Prediction error on true datas : 112.41385178520736\n",
      "[ 5300 , 29000 ] :Prediction error on true datas : 112.25640567337112\n",
      "[ 5300 , 31000 ] :Prediction error on true datas : 112.19278677394983\n",
      "[ 5300 , 33000 ] :Prediction error on true datas : 112.1836651010774\n",
      "[ 5300 , 35000 ] :Prediction error on true datas : 112.22083820858936\n",
      "\n",
      "[ 5400 , 25000 ] :Prediction error on true datas : 112.552938896958\n",
      "[ 5400 , 27000 ] :Prediction error on true datas : 112.43759246714741\n",
      "[ 5400 , 29000 ] :Prediction error on true datas : 112.27195442317773\n",
      "[ 5400 , 31000 ] :Prediction error on true datas : 112.20036138226463\n",
      "[ 5400 , 33000 ] :Prediction error on true datas : 112.1733943948199\n",
      "[ 5400 , 35000 ] :Prediction error on true datas : 112.20914510414147\n",
      "\n",
      "[ 5500 , 25000 ] :Prediction error on true datas : 112.49125007322272\n",
      "[ 5500 , 27000 ] :Prediction error on true datas : 112.3606312624197\n",
      "[ 5500 , 29000 ] :Prediction error on true datas : 112.18635218699355\n",
      "[ 5500 , 31000 ] :Prediction error on true datas : 112.10191851025193\n",
      "[ 5500 , 33000 ] :Prediction error on true datas : 112.06116762819983\n",
      "[ 5500 , 35000 ] :Prediction error on true datas : 112.08684138983473\n",
      "\n",
      "[ 5600 , 25000 ] :Prediction error on true datas : 112.45665440530388\n",
      "[ 5600 , 27000 ] :Prediction error on true datas : 112.324572642191\n",
      "[ 5600 , 29000 ] :Prediction error on true datas : 112.13433476975023\n",
      "[ 5600 , 31000 ] :Prediction error on true datas : 112.04366777464526\n",
      "[ 5600 , 33000 ] :Prediction error on true datas : 111.99049982201248\n",
      "[ 5600 , 35000 ] :Prediction error on true datas : 112.00679807202712\n",
      "\n",
      "[ 5700 , 25000 ] :Prediction error on true datas : 112.40335277427727\n",
      "[ 5700 , 27000 ] :Prediction error on true datas : 112.26541319390726\n",
      "[ 5700 , 29000 ] :Prediction error on true datas : 112.07366580701947\n",
      "[ 5700 , 31000 ] :Prediction error on true datas : 111.96765283270524\n",
      "[ 5700 , 33000 ] :Prediction error on true datas : 111.91489492728046\n",
      "[ 5700 , 35000 ] :Prediction error on true datas : 111.92209403448902\n",
      "\n",
      "[ 5800 , 25000 ] :Prediction error on true datas : 112.37886409413362\n",
      "[ 5800 , 27000 ] :Prediction error on true datas : 112.23293084621427\n",
      "[ 5800 , 29000 ] :Prediction error on true datas : 112.02835784354221\n",
      "[ 5800 , 31000 ] :Prediction error on true datas : 111.9240766803299\n",
      "[ 5800 , 33000 ] :Prediction error on true datas : 111.84795059006244\n",
      "[ 5800 , 35000 ] :Prediction error on true datas : 111.84782291968632\n",
      "\n",
      "[ 5900 , 25000 ] :Prediction error on true datas : 112.33225839883325\n",
      "[ 5900 , 27000 ] :Prediction error on true datas : 112.17950605082483\n",
      "[ 5900 , 29000 ] :Prediction error on true datas : 111.97011762196651\n",
      "[ 5900 , 31000 ] :Prediction error on true datas : 111.85881909408106\n",
      "[ 5900 , 33000 ] :Prediction error on true datas : 111.7796965049859\n",
      "[ 5900 , 35000 ] :Prediction error on true datas : 111.77906566312744\n",
      "\n",
      "[ 6000 , 25000 ] :Prediction error on true datas : 112.27841454420924\n",
      "[ 6000 , 27000 ] :Prediction error on true datas : 112.12126282524028\n",
      "[ 6000 , 29000 ] :Prediction error on true datas : 111.90757115370462\n",
      "[ 6000 , 31000 ] :Prediction error on true datas : 111.78706684069891\n",
      "[ 6000 , 33000 ] :Prediction error on true datas : 111.70004220632434\n",
      "[ 6000 , 35000 ] :Prediction error on true datas : 111.69740919256748\n",
      "\n",
      "Wall time: 47.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for lower_bound in range(4500, 6001, 100):\n",
    "    for upper_bound in range(25000, 35002, 2000):\n",
    "        X, X_big, X_low, y, y_big, y_low = prepare_data(lower_bound, upper_bound, tweets)\n",
    "        # Applying linear regression\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        lr = LinearRegression(fit_intercept=False)\n",
    "        lr_estimators = cross_validate(lr, X, y, return_estimator=True)['estimator']\n",
    "        lr = cv_worst(lr_estimators, X, y, verbose=False)\n",
    "        print('[', lower_bound, ',', upper_bound, \"] :\", end='')\n",
    "        error_true(lr, X_big, y_big, X_low, y_low)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "ransta = 0\n",
    "lower_bound = 50\n",
    "upper_bound = 2000\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=ransta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying SGD regression with huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 243.8984762540318\n",
      "Prediction error on true datas : 139.82123894336993\n",
      "Wall time: 5.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd = SGDRegressor(max_iter=1000, tol=1e-4, random_state=ransta, loss='huber')\n",
    "sgd_estimators = cross_validate(sgd, X, y, return_estimator=True)['estimator']\n",
    "sgd = cv_worst(sgd_estimators, X, y)\n",
    "error_true(sgd, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 276.52121009898786\n",
      "Prediction error on true datas : 139.95671974249638\n",
      "Wall time: 46.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "lr_estimators = cross_validate(lr, X, y, return_estimator=True)['estimator']\n",
    "lr = cv_worst(lr_estimators, X, y)\n",
    "error_true(lr, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying ridge regression : it does not improve the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 276.5223000778556\n",
      "Prediction error on true datas : 139.9572649701026\n",
      "Wall time: 63.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=0.5, fit_intercept=False, random_state=ransta)\n",
    "ridge_estimators = cross_validate(ridge, X, y, return_estimator=True)['estimator']\n",
    "ridge = cv_worst(ridge_estimators, X, y)\n",
    "error_true(ridge, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying bayesian ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 276.52405739072407\n",
      "Prediction error on true datas : 139.95749778078846\n",
      "Wall time: 61.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "bay = BayesianRidge(fit_intercept=False)\n",
    "bay_estimators = cross_validate(bay, X, y, return_estimator=True)['estimator']\n",
    "bay = cv_worst(bay_estimators, X, y)\n",
    "error_true(bay, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying generalized linear regression Power. Distribution\n",
    "0 : Normal.\n",
    "1 : Poisson.\n",
    "2 : Gamma.\n",
    "3 : Inverse Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 279.8728061394728\n",
      "Prediction error on true datas : 142.5287446096816\n",
      "Wall time: 222 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "gen = TweedieRegressor(power=2, alpha=1, link='log', fit_intercept=False)\n",
    "gen_estimators = cross_validate(gen, X, y, return_estimator=True)['estimator']\n",
    "gen = cv_worst(gen_estimators, X, y)\n",
    "error_true(gen, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying passive aggressive regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 241.97146034923813\n",
      "Prediction error on true datas : 139.22334204996568\n",
      "Wall time: 141 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "par = PassiveAggressiveRegressor(max_iter=100, random_state=ransta, tol=1e-3, fit_intercept=False)\n",
    "par_estimators = cross_validate(par, X, y, return_estimator=True)['estimator']\n",
    "par = cv_worst(par_estimators, X, y)\n",
    "error_true(par, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying gamma regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 278.37459681904124\n",
      "Prediction error on true datas : 140.09430785383094\n",
      "Wall time: 191 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import GammaRegressor\n",
    "gam = GammaRegressor(alpha=0.001)\n",
    "gam_estimators = cross_validate(gam, X, y, return_estimator=True)['estimator']\n",
    "gam = cv_worst(gam_estimators, X, y)\n",
    "error_true(gam, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying poisson regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 278.06869091313536\n",
      "Prediction error on true datas : 139.99452669587563\n",
      "Wall time: 222 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "poi = PoissonRegressor(alpha=0.001)\n",
    "poi_estimators = cross_validate(poi, X, y, return_estimator=True)['estimator']\n",
    "poi = cv_worst(poi_estimators, X, y)\n",
    "error_true(poi, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying k-nn regression : why does it fool the cross validation ??? Very sensitive to feature change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 27.86419889992903\n",
      "Prediction error on true datas : 144.75071532960737\n",
      "\n",
      "Prediction error on a train / test split : 126.74427974716298\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(n_neighbors=1)\n",
    "knn_estimators = cross_validate(knn, X, y, return_estimator=True)['estimator']\n",
    "knn = cv_worst(knn_estimators, X, y)\n",
    "error_true(knn, X_big, y_big, X_low, y_low)\n",
    "knn.fit(X_train, y_train)\n",
    "p = knn.predict(X_test).astype(int)\n",
    "print(\"\\nPrediction error on a train / test split :\", mean_absolute_error(y_true=y_test, y_pred=p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying logistic regression - I am not sure about what it does and how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction error on a train / test split : 111.02007171108565\n",
      "Prediction error on true datas : 146.0392008134856\n",
      "Wall time: 194 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "sample = 100\n",
    "logi = LogisticRegression() # penalty='l1', solver='saga', random_state=0, fit_intercept=False)\n",
    "logi.fit(X_train[:sample], y_train[:sample])\n",
    "p = logi.predict(X_test).astype(int)\n",
    "print(\"\\nPrediction error on a train / test split :\", mean_absolute_error(y_true=y_test, y_pred=p))\n",
    "error_true(logi, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying gradient boosting regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 235.76716716716717\n",
      "Prediction error on true datas : 138.66486526269307\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(loss='lad', criterion='friedman_mse', random_state=ransta)\n",
    "gbr_estimators = cross_validate(gbr, X, y, return_estimator=True)['estimator']\n",
    "gbr = cv_worst(gbr_estimators, X, y)\n",
    "error_true(gbr, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying neural network (MLP regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error on a train / test split : 98.47406941928807\n",
      "Prediction error on true datas : 143.59846915709014\n",
      "Wall time: 41.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "mlp = Sequential()\n",
    "input_shape = X.iloc[0].shape\n",
    "mlp.add(Dense(16, input_shape=input_shape, activation='relu'))\n",
    "mlp.add(Dense(8, activation='relu'))\n",
    "mlp.add(Dense(1, activation='linear'))\n",
    "mlp.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "mlp.fit(X_test, y_test, epochs=10, batch_size=10, verbose=0, validation_split=0.2)\n",
    "p = mlp.predict(X_test).astype(int)\n",
    "print(\"Prediction error on a train / test split :\", mean_absolute_error(y_true=y_test, y_pred=p))\n",
    "error_true(mlp, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying SVM regression (too long, but seems to have nice results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 97.06281050390348\n",
      "Prediction error on true datas : 143.40786930158296\n",
      "Wall time: 39.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import svm\n",
    "sample = 5000\n",
    "svmr = svm.SVR(kernel='poly')\n",
    "svmr_estimators = cross_validate(svmr, X[:sample], y[:sample], return_estimator=True)['estimator']\n",
    "svmr = cv_worst(svmr_estimators, X, y)\n",
    "error_true(svmr, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying random forest regression with mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error : 117.40592840667139\n",
      "Prediction error on true datas : 144.85139018019547\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=2, random_state=ransta, criterion='mse')\n",
    "rf_estimators = cross_validate(rf, X, y, return_estimator=True)['estimator']\n",
    "rf = cv_worst(rf_estimators, X, y)\n",
    "rf_pred = (rf.predict(X_test)).astype(int)\n",
    "error_true(rf, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying random forest regression with mae - poor implementation in sklearn with complexity in O(n^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=1, max_depth=1, random_state=ransta, criterion='mae')\n",
    "rf_estimators = cross_validate(rf, X, y, return_estimator=True)['estimator']\n",
    "rf = cv_worst(rf_estimators, X, y)\n",
    "rf_pred = (rf.predict(X_test)).astype(int)\n",
    "val, occ = np.unique(rf_pred, return_counts=True)\n",
    "print('\\n', val, \"with respecting occurences\", occ)\n",
    "error_true(rf, X_big, y_big, X_low, y_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.insert(3, 'stat_sqr', X.user_statuses_count ** 2, True)\n",
    "#X.insert(4, 'followers_sqr', X.user_followers_count ** 2, True)\n",
    "#X.insert(5, 'verified_exp', np.exp(X.user_verified), True)\n",
    "\n",
    "#X_big.insert(3, 'stat_sqr', X_big.user_statuses_count ** 2, True)\n",
    "#X_big.insert(4, 'followers_sqr', X_big.user_followers_count ** 2, True)\n",
    "#X_big.insert(5, 'verified_exp', np.exp(X_big.user_verified), True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
